# StudyAsist â€“ Plan & Design: Study Tools (Voice, Dictate, Explain, Solve)

**Version:** 1.0  
**Scope:** Change Voice (Settings), Dictate, Explain, Solve

---

## 1. Overview

| Feature       | Purpose | Input | Output | Network |
|---------------|---------|--------|--------|---------|
| **Change Voice** | Set TTS voice for alarm/speech | Settings UI | Applied to all TTS (alarm, Dictate, Explain, Solve) | No |
| **Dictate**   | Read aloud text from a photo   | Camera / upload image | OCR â†’ text â†’ TTS in chosen language | Optional (OCR) |
| **Explain**   | Explain text or image content | Text and/or image (camera/file) | AI explanation in language | Yes (AI API) |
| **Solve**     | Step-by-step solution to a problem | Text and/or image (camera/file) | AI step-by-step solution | Yes (AI API) |

---

## 2. Change Voice (Settings)

### 2.1 Goal
Let the user pick the **speech voice** (and optionally style) used for:
- Alarm TTS message
- Dictate (read-aloud)
- Explain / Solve (read-aloud of AI response, if offered)

### 2.2 Technical Approach

- **Android TextToSpeech** already supports multiple voices: `TextToSpeech.getVoices()` returns `Set<Voice>` (name, locale, quality, latency).
- **Settings:** New section â€œSpeech / Voiceâ€ with a single selector: **Voice** (dropdown or list of available voices for the default locale + optionally â€œSystem defaultâ€).
- **Persistence:** DataStore key e.g. `tts_voice_name` (String?) â€” store `Voice.name` or null for default. Optional: `tts_locale` (String?) for e.g. `"en-US"` if we want to filter voices by locale.
- **Apply everywhere:** When creating or using `TextToSpeech`, call `setVoice(voice)` before speaking (in `ReminderAlarmActivity`, `AlarmTtsService`, and any new Dictate/Explain/Solve TTS). If saved voice is null or unavailable, fall back to default (e.g. `setLanguage(Locale.getDefault())` and do not set voice).

### 2.3 Settings UI (wireframe)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â†  Settings                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ... (existing: Notifications, Sound,  â”‚
â”‚       Vibration, Alarm TTS message)     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Speech                                 â”‚
â”‚  Voice for alarms & reading             â”‚
â”‚  [System default                    â–¾]  â”‚  â† or list: English (US) - Female, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.4 Data / Code Touchpoints

- **SettingsDataStore:** Add `ttsVoiceName: stringPreferencesKey("tts_voice_name")` (nullable).
- **SettingsRepository:** `getTtsVoiceName(): Flow<String?>`, `setTtsVoiceName(String?)`. Optionally expose `availableVoices(): List<Voice>` (from TTS engine) for the picker.
- **NotificationScheduler / ReminderAlarmActivity / AlarmTtsService:** When starting TTS, resolve saved voice name to `Voice` and call `tts.setVoice(voice)` if found.
- **New:** Use the same resolved voice in Dictate, Explain, and Solve when they use TTS.

---

## 3. Dictate

### 3.1 Goal
User captures or uploads an **image** of a paragraph/chapter â†’ app **extracts text (OCR)** â†’ **reads it aloud** in the chosen language (TTS).

### 3.2 Flow

1. User opens **Dictate** screen.
2. **Input:** â€œTake photoâ€ (camera) or â€œUpload imageâ€ (gallery / file picker).
3. User selects or captures an image.
4. User taps **â€œDictateâ€** (or â€œRead aloudâ€).
5. App runs **OCR** on the image â†’ plain text.
6. Optional: **Language** selector (e.g. â€œLanguage for readingâ€) so TTS uses that locale.
7. App uses **TTS** to read the text (respecting **Change Voice** setting). Optional: show extracted text in a scrollable area and highlight sentence/word while reading (stretch goal).

### 3.3 Technical Choices

| Component | Recommendation | Alternative |
|-----------|----------------|-------------|
| **OCR** | **ML Kit Text Recognition** (Google, on-device) | Google Cloud Vision API (network); Tesseract (heavier, on-device) |
| **Image source** | Camera (CameraX or Activity result) + Gallery (ActivityResultContracts.GetContent for image/*) | Same |
| **TTS** | Android `TextToSpeech` (existing) with selected voice and language | â€” |
| **Language** | User choice in Dictate screen (e.g. dropdown: English, Hindi, etc.) stored per-screen or in DataStore for â€œDictate languageâ€ | Auto-detect from OCR (complex, can add later) |

### 3.4 Permissions

- **Camera:** `android.permission.CAMERA` (runtime for API 23+).
- **Read storage / media:** For gallery pick, use `READ_MEDIA_IMAGES` (API 33+) or `READ_EXTERNAL_STORAGE` (legacy); `ActivityResultContracts.GetContent()` often avoids storage permission on modern Android.

### 3.5 Dictate UI (wireframe)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â†  Dictate                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Add image of paragraph or chapter      â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  ğŸ“·         â”‚  â”‚  ğŸ“         â”‚      â”‚
â”‚  â”‚  Take photo â”‚  â”‚  Upload     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                         â”‚
â”‚  [  Preview of selected image  ]        â”‚
â”‚                                         â”‚
â”‚  Language for reading    [English   â–¾]  â”‚
â”‚                                         â”‚
â”‚  [        Dictate (Read aloud)        ] â”‚
â”‚                                         â”‚
â”‚  â”€â”€â”€ Extracted text (optional) â”€â”€â”€      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ The quick brown fox...           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.6 Architecture

- **Screen:** `DictateScreen` (Compose).
- **ViewModel:** `DictateViewModel` â€” holds image URI/bitmap, runs OCR in a coroutine (e.g. `Dispatchers.Default`), exposes extracted text + state (loading, error), triggers TTS (or exposes â€œtext to speakâ€ for a TTS helper).
- **Use case / repository:** Optional `OcrRepository` or `TextRecognitionHelper` wrapping ML Kit; called from ViewModel. TTS can be a shared `TtsHelper` that takes text + locale + voice and speaks (used by Dictate, Explain, Solve).

---

## 4. Explain

### 4.1 Goal
User provides **text** and/or **image** (camera or upload) â†’ app sends content to an **AI** service with a prompt like â€œExplain this in simple terms in [language]â€ â†’ show **explanation** and optionally read it aloud.

### 4.2 Flow

1. User opens **Explain** screen.
2. **Input:**  
   - Text: multiline field â€œPaste or type text to explainâ€.  
   - Image: â€œTake photoâ€ / â€œUpload imageâ€ (if image: run OCR first, then send resulting text to AI; or send image if API supports vision).
3. User optionally selects **language** for the explanation (e.g. â€œExplain in: [English â–¾]â€).
4. User taps **â€œExplainâ€**.
5. App sends request to **AI API** (see below) with combined text (and optionally image).
6. Show **explanation** in a scrollable area; optional **â€œRead aloudâ€** button using TTS (with Change Voice).

### 4.3 Technical Choices

| Component | Recommendation | Notes |
|-----------|----------------|-------|
| **AI API** | **Google Gemini** (Gemini API, supports text + image, good for mobile) or **OpenAI** (GPT-4o for vision) | Requires API key; store in BuildConfig or secure storage; network required |
| **Input** | Text field + image (camera/gallery). If image: OCR (ML Kit) â†’ text, then send text to AI; or use Vision API (Gemini/GPT-4o) with image URL/base64 | Vision reduces OCR dependency; OCR keeps one path for â€œtext onlyâ€ from image |
| **Language** | User selector â€œExplain in: [locale]â€ in the prompt | Simple and explicit |

### 4.4 Explain UI (wireframe)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â†  Explain                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Add text or image to explain           â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Type or paste text here...           â”‚â”‚
â”‚  â”‚                                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  ğŸ“· Photo   â”‚  â”‚  ğŸ“ Upload   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                         â”‚
â”‚  Explain in         [English        â–¾]  â”‚
â”‚                                         â”‚
â”‚  [            Explain                  ]â”‚
â”‚                                         â”‚
â”‚  â”€â”€â”€ Explanation â”€â”€â”€                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ This paragraph describes...         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  [       Read aloud       ]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.5 Architecture

- **Screen:** `ExplainScreen`.
- **ViewModel:** `ExplainViewModel` â€” input text, image URI, language; calls a **repository** or **use case** that talks to the AI API (Gemini/OpenAI); exposes explanation text + loading/error; optional â€œread aloudâ€ triggers TTS.
- **Backend:** `ExplainRepository` or `AiRepository` â€” one method e.g. `suspend fun explain(text: String, imageUriOrBytes: Any?, language: String): Result<String>`. Use Retrofit/OkHttp or Gemini SDK for Android. API key from BuildConfig or user input in Settings (advanced).

---

## 5. Solve

### 5.1 Goal
User provides a **problem** as text and/or image â†’ app sends it to an **AI** with a prompt like â€œSolve this step by step in [language]â€ â†’ show **step-by-step solution**; optional read aloud.

### 5.2 Flow
Same as Explain, but:
- Prompt: â€œSolve this problem step by step. Explain each step clearly. Output in [language].â€
- Output: Step-by-step solution (markdown or plain text); optional TTS.

### 5.3 Technical Choices
- **Same AI API** as Explain (Gemini or OpenAI).
- **Same input** (text + optional image via OCR or vision).
- **Repository:** `SolveRepository` or extend `AiRepository` with `solve(problemText: String, image: Any?, language: String): Result<String>`.

### 5.4 Solve UI (wireframe)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â†  Solve                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Add problem (text or image)            â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Enter or paste the problem...        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  ğŸ“· Photo   â”‚  â”‚  ğŸ“ Upload   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                         â”‚
â”‚  Solution language  [English        â–¾]  â”‚
â”‚                                         â”‚
â”‚  [            Solve                     ]â”‚
â”‚                                         â”‚
â”‚  â”€â”€â”€ Step-by-step solution â”€â”€â”€          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Step 1: ...                          â”‚â”‚
â”‚  â”‚ Step 2: ...                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  [       Read aloud       ]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Navigation & Entry Points

- **Home** (or a new â€œStudyâ€ tab): Add entry points to **Dictate**, **Explain**, **Solve** (e.g. cards or a bottom nav / drawer item â€œStudy toolsâ€ that opens a sub-menu or a single â€œToolsâ€ screen with three buttons).
- **NavRoutes:** Add `DICTATE`, `EXPLAIN`, `SOLVE` (e.g. `"dictate"`, `"explain"`, `"solve"`).
- **Settings:** Add â€œSpeech / Voiceâ€ section (Change Voice); optionally â€œAPI keyâ€ or â€œAI providerâ€ for Explain/Solve (if key is user-provided).

Suggested navigation:

```
Home (Today | Timetables)
   â”œâ”€â”€ Settings (existing + Change Voice)
   â””â”€â”€ Study tools (new)
         â”œâ”€â”€ Dictate
         â”œâ”€â”€ Explain
         â””â”€â”€ Solve
```

---

## 7. Dependencies (Gradle)

| Feature   | Dependency | Purpose |
|-----------|------------|---------|
| Dictate   | ML Kit Text Recognition | OCR on-device |
| Explain   | Retrofit + OkHttp, or Gemini Android SDK | Call Gemini/OpenAI API |
| Solve     | Same as Explain | Same API, different prompt |
| Change Voice | None (built-in TTS) | Use existing TextToSpeech APIs |

- **ML Kit:** `com.google.mlkit:text-recognition` (and optionally language-specific modules, e.g. Latin, Devanagari).
- **Network:** For Gemini: `com.google.ai.client.generativeai:generativeai` or REST; for OpenAI: `com.squareup.retrofit2:retrofit` + JSON converter. API key must not be committed; use `BuildConfig` or Settings.

---

## 8. DataStore / Settings Additions

| Key | Type | Use |
|-----|------|-----|
| `tts_voice_name` | String? | Change Voice: selected TTS voice name |
| `dictate_language` | String? | Default â€œDictateâ€ reading language (e.g. "en") |
| `explain_language` | String? | Default â€œExplain inâ€ language |
| `solve_language` | String? | Default â€œSolution languageâ€ |
| `ai_api_key` | String? | Optional: user-entered API key for Explain/Solve |
| `ai_provider` | String? | Optional: "gemini" | "openai" |

---

## 9. Implementation Order

1. **Change Voice** â€” Settings + DataStore + apply in existing TTS (alarm, service). No new screens beyond Settings.
2. **Dictate** â€” Screen, camera/gallery, ML Kit OCR, TTS; use Change Voice when available.
3. **Explain** â€” Screen, text + image input, AI API integration, explanation UI, optional TTS.
4. **Solve** â€” Screen, same input as Explain, AI with â€œstep by stepâ€ prompt, solution UI, optional TTS.

---

## 10. Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| API key exposure | Store in BuildConfig (build-time) or secure pref; never log or send to third parties except the chosen AI provider |
| No network for Explain/Solve | Show clear â€œNo connectionâ€ / â€œAPI errorâ€; optional offline message |
| OCR quality on poor photos | Show extracted text so user can edit before Dictate; optional â€œRetakeâ€ |
| TTS voice not available after OS update | Fallback to default voice if saved voice name not in `getVoices()` |
| Large images for AI | Resize/compress before upload; or send only OCR text for Explain/Solve to reduce payload |

---

## 11. Optional Enhancements (Later)

- **Dictate:** Highlight current sentence/word during TTS (using `UtteranceProgressListener` and word boundaries).
- **Explain/Solve:** Support markdown in AI response (e.g. `BasicMarkdown` or simple formatting in Compose).
- **Offline:** Prefer on-device OCR (ML Kit) for Dictate; Explain/Solve remain online unless an on-device model is integrated later.
- **History:** Save last Dictate/Explain/Solve inputs or results in Room (optional table) for â€œRecentâ€ or â€œFavoritesâ€.

This plan keeps the app consistent with existing MVVM, Compose, DataStore, and Hilt, and reuses TTS and (for Explain/Solve) a single AI client with two prompt types.
